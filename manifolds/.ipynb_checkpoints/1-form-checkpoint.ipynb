{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed1866c-abf7-441e-84fe-aac5f5e10b1a",
   "metadata": {},
   "source": [
    "(sec:1-forms)=\n",
    "# 1-forms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581525b9-aeb0-495c-b101-54d7e331ae8e",
   "metadata": {},
   "source": [
    "(sec:gradient)=\n",
    "## Gradient\n",
    "\n",
    "Gradient of scalar functions is an essential operation in vector calculus.  In the Euclidean space, it is defined as\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla f = \\partial_x f\\, \\mathbb{e}_x + \\partial_y f\\, \\mathbb{e}_y + \\partial_z f\\, \\mathbb{e}_z\n",
    "$$\n",
    "\n",
    "\n",
    "In math or physics course, we learned that the the gradient of scalar function is a \"vector\" field.  We want to generalize it to curved spaces or general manifolds.  After deep thought, mathematicians and theoretical physicists reached a conclusion that the gradient is not a vector field but something else.  You will see why it is not a vector when we investigate the properties of various physical quantities in later chapters.\n",
    "\n",
    "For a smooth function $f$ on manifold $M$, mathematicians introduced a map $df: \\text{Vect}(M) \\rightarrow \\mathbb{C}^\\infty (M)$.  This expression is a bit confusing.  You can think of $d$ as an operator acting on $f$ and then $d$ and $f$  jointly form a map $df$.  The map takes a vector field $v \\in \\text{Vect}(M)$ and outputs a  smooth function $v(f)$:\n",
    "\n",
    "\n",
    "$$\n",
    "df(v) = v(f)\n",
    "$$\n",
    "\n",
    "\n",
    "which means that $df(v)$ and $v(f)$ outputs the same function.  Then, why do we need two maps?  While the vector field takes a function as an input, $df$ takes a vector field as an input.  In other words, in $v(f)$, $v$ is specifically given and $f$ is a \"variable\" and In $df$, $f$ is specifically given and $v$ is a \"variable\".  You will have a better  undestanding by working on a chart. See {numref}`sec:differential-on-a-chart`.\n",
    "\n",
    "In order to guarantee that the gradient does not depend on the choice of a vector field, we require that $df$ satisfies  te following linearity conditions:   for $v, w \\in \\text{Vect}(M)$ and $g \\in \\mathbb{C}^\\infty(M)$ \n",
    "\n",
    "\n",
    "$$\n",
    "df(v+w) = df(v) + df(w), \\qquad df(gv) = g df(v).\n",
    "$$\n",
    "\n",
    "\n",
    "Do not get confused with the addition of two differentials  There is only one differential $df$ in the above relations.  \n",
    "\n",
    "$df$ is called the `differential` of $f$, or the `exterior derivative` of $f$.  It is an example of `1-form` which we introduce below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f59d3-6cff-4bf8-8a26-5c879efa40f4",
   "metadata": {},
   "source": [
    "(sec:differential-on-a-chart)=\n",
    "## Differential on a chart\n",
    "\n",
    "Working on a chart provides with a better understanding of the differential. Evaluate it on $\\mathbb{R}^n$ through a chart $\\phi: U \\in M \\rightarrow \\mathbb{R}^n$,    the differential is expressed as\n",
    "\n",
    "\n",
    "$$\n",
    "df(v) = v(f) = v^\\mu(x) \\partial_\\mu f(x)\n",
    "$$\n",
    "\n",
    "\n",
    "This is nothing but a directional derivative.  Are $v(f)$  and $df(v)$ the same thing?  They are not.  The vector field $v(f)$ tries to find the coordinate expression of the vector field $v_\\mu(x)$.  Any function can be  used and $v^\\mu(x)$ is independent from the choice of $f$.  On the other hand,  $df(v)$ tries to evaluate the gradient of a given function $f$ with help of a vector field $v^\\mu(x)$.  Any vector field can be used and the gradient is independent of  a choice of vector field $v$.\n",
    "\n",
    "The linearity conditions guarantees it.\n",
    "\n",
    "\n",
    "$$\n",
    "df(v+w) = (v+w)^\\mu (\\partial_\\mu f)_{v+w} = (v^\\mu + w^\\mu) (\\partial_\\mu f)_{v+w}\n",
    "$$\n",
    "\n",
    "\n",
    "where $(\\partial_\\mu f)_{v+w}$ indicates the gradient is evaluated with $v+w$.\n",
    "\n",
    "\n",
    "$$\n",
    "df(v)+df(w) = v^\\mu (\\partial_\\mu f)_v + w^\\mu (\\partial_\\mu f)_w\n",
    "$$\n",
    "\n",
    "\n",
    "Since we can chose any $v$ and $w$, we concluse  that $(\\partial_\\mu f)_{v+w} = (\\partial_\\mu f)_v = (\\partial_\\mu f)_w$, indicating that the grdianrt does not depend on the choise of a vector field.\n",
    "\n",
    "Working on a chart,  the above definition of $df$  agrees with the total differential of function,\n",
    "\n",
    "\n",
    "$$\n",
    "df = \\partial_\\mu f dx^\\mu\n",
    "$$ (eq:total-differential)\n",
    "\n",
    "\n",
    "which we learned in a calculus curse.\n",
    "\n",
    "\n",
    "```{prf:proof}\n",
    "\n",
    "The differential  of $x^\\mu$ is calculated as\n",
    "\n",
    "$$\n",
    "d x^\\mu (v)  = v(x^\\mu) = v^\\nu \\partial_\\nu x^\\mu = v^\\nu \\delta^\\mu_\\nu = v^\\mu\n",
    "$$\n",
    "\n",
    "Substituting it to the differential of $f$,\n",
    "\n",
    "$$\n",
    "d x^\\mu (v)  = v(x^\\mu) = v^\\nu \\partial_\\nu x^\\mu = v^\\nu \\delta^\\mu_\\nu = v^\\mu\n",
    "$$\n",
    "\n",
    "Since this equality holds for any $v$, we obtain {numref}`eq:total-differential`\n",
    "\n",
    "```\n",
    "\n",
    "It is natural to call $df$ on $M$ the differential of $f$.\n",
    "\n",
    "Recall that $\\partial_\\nu$ is a basis of vecgtor fields in $\\mathbb{R}^n$.  That means $\\partial_\\nu$ is a vector filed and can be fed to the differential.\n",
    "\n",
    "\n",
    "$$\n",
    "dx ^\\mu(\\partial_\\nu) = \\partial_\\nu(x^\\mu) = \\delta^\\mu_\\nu\n",
    "$$ (eq:omega-v-orthogonality)\n",
    "\n",
    "\n",
    "How can we find the partial derivative $\\partial_\\mu f$?   In the directional derivative, the vector field $v$ specifies the direction of the derivative.  If we want to know the derivative along a coordinate axis, the direction can be specified with a basis vector $\\partial_\\mu$.  So feed the vector field $v = \\partial_\\mu$ to the differential\n",
    "\n",
    "\n",
    "$$\n",
    "df(\\partial_\\mu) = \\partial_\\mu f\n",
    "$$ (eq:partial-derivative)\n",
    "\n",
    "\n",
    "Now, we are sure that $df$ generates the gradient of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ad27f-41a3-475b-9cec-288f3b8f4ec5",
   "metadata": {},
   "source": [
    "(sec:1-form)=\n",
    "## 1-forms\n",
    "\n",
    "We generalize the above idea.  Consider a map $\\omega: \\text{Vect}(M) \\rightarrow \\mathbb{C}^\\infty(M)$ such that for $v, w \\in \\text{Vect}(M)$ and $g \\in \\mathbb{C}^\\infty(M)$\n",
    "\n",
    "$$\n",
    "\\omega(v+w) = \\omega(v) + \\omega(w), \\qquad \\omega(gv) = g \\omega(v)\n",
    "$$\n",
    "\n",
    "We call $\\omega$ a `1-form.` Clearly, $\\omega=df$ satisfies the above linearity conditions  Thus, $df$ is a 1-form.   The addition and scalar multiplication can be defined for $v, w \\in \\text{Vect}(M)$ and $g \\in \\mathbb{C}^\\infty$,\n",
    "\n",
    "$$\n",
    "(\\omega + \\mu) (v) = \\omega(v) + \\mu(v), \\qquad (g\\omega)(v) = g \\omega(v)\n",
    "$$\n",
    "\n",
    "Since the addition and the scalar multiplication obey th axioms of vector, 1-forms behave like vectors.  Because of this,  some people call 1-form `covector` or `dual vector`.  In this lecture note, we stick to 1-form.\n",
    "\n",
    "Notice that no function $f$ is used to define the 1-form.  Hence, $\\omega$ does not look like a gradient anymore.  However, it is just a generalization of gradient.  Recall that the vector field is originally defined as $v(f) = V^\\mu \\partial_\\mu f$ but we simply write the field as $v = V^\\mu \\partial_\\mu$ without $f$.  Then we interpreted $\\partial_\\mu$ as the basis of the vector field.  It does make a sense since a vector field should not depend on $f$.  We are trying to do the same for 1-form.  You can think that we are trying to find a gradient operator like $\\nabla$ on $M$.  When it acts on $f$, you get a gradient.\n",
    "\n",
    "By analogy from {eq}`eq:partial-derivative`,  we define a component of 1-form by \n",
    "\n",
    "$$\n",
    "\\omega_\\mu = \\omega(\\partial_\\mu).\n",
    "$$ (eq:1-form-component)\n",
    "\n",
    "\n",
    "\n",
    "and based on {eq}`eq:total-differential`, \n",
    "\n",
    "\n",
    "$$\n",
    "\\omega = \\omega_\\mu dx^\\mu.\n",
    "$$ (eq:1-form-decomposition)\n",
    "\n",
    "\n",
    "If $\\omega = df$, then we recover {eq}`eq:total-differential` and {eq}`eq:partial-derivative`.   Equation {eq}`eq:1-form-decomposition` suggests that $\\{ dx^\\mu\\}$ is a basis set sapnning of 1-forms on $\\mathbb{R}^n$ much like $\\partial_\\mu$ forms a basis set for vector fields.  Again, 1-forms look like vectors.\n",
    "\n",
    "## Inner products vs duarity paring\n",
    "\n",
    "Before going to defineinner products on a manifold, we review traditional  inner products we have been using in the Euclidean space.   We often think of inner product (dot product) between two vectors.  For two vectors $\\vec{a} = \\sum_i a^i \\mathbb{e}^i$ and $\\vec{b} = \\sum_i b^i \\mathbb{e}^i$ where $\\{\\mathbb{e}_i\\}$ is orthonormal vasis vectors,  their inner product is simply $\\vec{a} \\cdot \\vec{b} =\\sum_i a^i b^i$ as seen in freshman physics courses.  The orthonormality  $\\mathbb{e}_i \\cdot \\mathbb{e}_j = \\delta_{ij}$ is essential.   This inner product between two Euclidean vectors is actually a rather special case. It is a map from a product space $V \\times V$ to $\\mathbf{R}$ where $V$ is a vector space spanned by an orthonormal basis set $\\{\\mathbb{e}_i\\}$.\n",
    "\n",
    "However, in many cases, inner product is actually between a vector and a dual vector.   In other words, it is a map from $V^\\dagger \\times V$ to $\\mathbb{R}$ where $V^\\dagger$ is another vector space dual to $V$.   For example, if  vectors $A, B \\in V$ are expressed as column matrices, the inner product is $A^T B$ where $A^T$ is a transpose of $A$.  The transposed vector $A^T \\in V^\\dagger$ is a dual vector to $A$.  In quantum mechanics, inner product is defined between a ket vector $|\\psi\\rangle$ and a bra vector $\\langle \\phi|$ where the bra vector $\\langle \\phi|$ is dual to the ket vector $|\\phi\\rangle$.\n",
    "\n",
    "The use of dual vectors becomes essential when the basis vectors are not onrthogonal to each other.  Let $\\qquad g_{ij} \\equiv \\mathbb{e}_i \\cdot \\mathbb{e}_j$.  The inner product becomes \n",
    "\n",
    "$$\n",
    "\\vec{a}\\cdot\\vec{b} = \\sum_{i,j} g_{ij} a^i b^j\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where $g_{ij}$ is called metric tensor which we will discus later.\n",
    "\n",
    "If we define a dual vector \n",
    "\n",
    "$$\n",
    "a_j = g_{ij} a^i\n",
    "$$ (eq:dual-vector)\n",
    "\n",
    "\n",
    "Noptice that the location of the inde moved from top to bottom.   With this transformation  then the expression of the product is much simpler, $\\vec{a} \\cdot \\vec{b} = sum_j a_jb^j$ or just $a_j b^j$ if the Einstein's convention is used.  The use of dual vectors indeed simplfies the expression of inner product because the metric information is hidden in the dual vectors.  A similar situation can be seen in solid state physics.  The  lattice vectors are expressed with basis vectors $\\mathbb{e}_i$ defined with a unit cell of the crystal.  Unless the unit cell is cubic,  the basis vectors are not orthnolomal to each other .  In order to avoid  the complicated expression of inner products, reciprocal basis vectors $\\hat{\\mathbb{e}}^i$ is introduced such that $\\hat{\\mathbb{e}}^i \\cdot \\mathbb{e}_j = \\delta_{j}^{i}$.\n",
    "\n",
    "Now we look at 1-form again. In the Euclidean space, the directional derivative $v(f) = v^\\mu(x) \\partial_\\mu f(x)$ is an inner product between vector field $v(x)$ and  gradient $\\nabla f(x)$,  The gradient on a manifold corresponds to a 1-form.  Furthremore, Eq. {eq}`eq:omega-v-orthogonality` suggests that the basis of vector and the vasis of 1-form looks like orthogonal similar to lattice-vs-reciprocal relation in a crystal.   So, it is natural to think of  inner prodcut between vector field $v$ and 1-form $\\omega$.    In fact, \\omega(v)$  likes like an innder product as you can see in the following expression:\n",
    "\n",
    "$$\n",
    "\\omega(v) = \\omega_\\mu dx^\\mu(v^\\nu \\partial_\\nu) = \\omega_\\mu v^\\nu \\delta^\\mu_\\nu = \\omega_\\mu v^\\mu\n",
    "$$ (eq:duarity-pair)\n",
    "\n",
    "\n",
    "But is this an inner product?  Actually it is not a true inner product because $\\omega$ is not dual to $v$.  There is no relation between $\\omega$ and $v$ .  More precisely, no metric tensor is defined at the present stage and thus we cannot construct a dual of $v$ like {eq}`eq:dual-vector`.  We need to wait for  metric tensor defined in a later chapter.\n",
    "\n",
    "Instead we call {eq}`eq:duarity-pair` `duality pair` or `contraction` and denote it as $\\langle \\omega,v\\rangle \\equiv \\omega (v)$. WE will comback to this issue later.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book",
   "language": "python",
   "name": "book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
